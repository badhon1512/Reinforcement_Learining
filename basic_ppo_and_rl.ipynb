{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3859f90b-0d3e-44b6-8324-e622e4a0cea0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reinforment learing\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "488e1d62-2cb1-491c-8c91-978496d2457b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb86ac-b906-4b5d-9c7a-7f1ed017b518",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c2c4d4e-10f4-48ba-b7ce-5e8dc16a03ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ffd887-42d6-49f4-a568-47faaefaa7fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:18.0\n",
      "Episode:2 Score:12.0\n",
      "Episode:3 Score:18.0\n",
      "Episode:4 Score:19.0\n",
      "Episode:5 Score:45.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "from pathlib import Path\n",
    "\n",
    "log_dir= \"runs/\",\n",
    "\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Display the image\n",
    "        action = env.action_space.sample()\n",
    "        state, r, done, info= env.step(action)\n",
    "        reward+=r\n",
    "    print('Episode:{} Score:{}'.format(episode, reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc8acc9-5fd4-4310-b2f0-02c00c42cea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainings/logs'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join('trainings','logs')\n",
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155c78b8-f496-474b-abbe-3dcaa353063c",
   "metadata": {},
   "source": [
    "creating and training the RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95ba5f6-6edc-434c-b15a-89dcd823598d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badhon/miniconda3/envs/sofa/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to trainings/logs/PPO_6\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1251 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1013        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008687853 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00767     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.64        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 49.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1012        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008577872 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.0454      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.17        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010385796 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 974         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008944867 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.253       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f3d28341190>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env = DummyVecEnv([lambda: env])\n",
    "m = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "m.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f93eadc-d070-49f8-b8f5-258efabb0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('trainings','models','CartPole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0aac50b9-b7f2-495d-9ff7-d0dc9848e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be08dafd-a070-4aad-a9fe-93aa1da82058",
   "metadata": {},
   "outputs": [],
   "source": [
    "del m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3c8d6700-6ea4-479b-966d-9959615b905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PPO.load(model_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5cf6783-d283-4ba9-88b0-dbcaac8f586c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f6f6c3e9e50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e335412c-1536-4a88-ace2-f4f572b96458",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d44b67f-5b0a-4156-85ea-186e5e35e1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(394.95, 104.77951851387752)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(m, env, n_eval_episodes=20, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fd41e69-c45e-4c1d-bca4-e70fb8ca243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca574dc-68be-43c1-8b48-0e10607e7a31",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f56423a-fcbe-433b-9e89-c68bb38bcaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[191.]\n",
      "Episode:2 Score:[102.]\n",
      "Episode:3 Score:[106.]\n",
      "Episode:4 Score:[160.]\n",
      "Episode:5 Score:[182.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    reward = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Display the image\n",
    "        action, _ = m.predict(obs)\n",
    "        obs, r, done, info= env.step(action)\n",
    "        reward+=r\n",
    "    print('Episode:{} Score:{}'.format(episode, reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532d151-197d-4526-b822-812aab6e7059",
   "metadata": {},
   "source": [
    "Tensor Board Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec09aa1-5fcc-4123-960a-76a40f216135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trainings/logs/PPO_2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_log_path = os.path.join(log_path,'PPO_2')\n",
    "t_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dba01a-8ff5-47f1-b592-aecc6b6fea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir=(t_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f42d6a-ff37-4c88-a87b-e8b57e817a5f",
   "metadata": {},
   "source": [
    "callback for training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f01726e6-3147-4352-b24d-ff84ac38091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = os.path.join('trainings','models')\n",
    "\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=180, verbose=1)\n",
    "eval_callback = EvalCallback(env, callback_on_new_best = stop_callback, eval_freq=10000, best_model_save_path=save_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "453beb65-5930-4be1-a7f9-c16a5b60da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy',env,verbose=1, tensorboard_log= log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97634714-2865-4e78-89f2-e5581da6d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to trainings/logs/PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1744 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1300        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008561747 |\n",
      "|    clip_fraction        | 0.0956      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00196    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 52.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1208        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009459154 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0903      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 38.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1121        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008619467 |\n",
      "|    clip_fraction        | 0.083       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.1        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 56.3        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=284.20 +/- 79.24\n",
      "Episode length: 284.20 +/- 79.24\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 284         |\n",
      "|    mean_reward          | 284         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342662 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.7        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    value_loss           | 74.2        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 284.20  is above the threshold 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f3d282fea60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps =20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb3fe2b-bfb6-4fbc-a3be-83a9e96bd516",
   "metadata": {},
   "source": [
    "Chaning the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e06914c-f038-4cd3-af01-19fb89d58ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi = [128,128], vf=[128,128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b2c1074-5b93-4202-bce1-d85ab5a4baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badhon/miniconda3/envs/sofa/lib/python3.9/site-packages/stable_baselines3/common/policies.py:460: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy',env,verbose=1, tensorboard_log= log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b368de56-04e1-4254-9674-615951fe0b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to trainings/logs/PPO_10\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1653 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1241         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072872033 |\n",
      "|    clip_fraction        | 0.0651       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.59        |\n",
      "|    explained_variance   | 0.494        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.7         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 49.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1149         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035695664 |\n",
      "|    clip_fraction        | 0.0604       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.574       |\n",
      "|    explained_variance   | 0.641        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.34         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00971     |\n",
      "|    value_loss           | 38.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1109         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074731456 |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.89         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0116      |\n",
      "|    value_loss           | 16.6         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=483.20 +/- 33.60\n",
      "Episode length: 483.20 +/- 33.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 483          |\n",
      "|    mean_reward          | 483          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068000704 |\n",
      "|    clip_fraction        | 0.0568       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 18.4         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 483.20  is above the threshold 180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f3d265978b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea734aa-f4f0-41a8-8712-33221ffd6c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
